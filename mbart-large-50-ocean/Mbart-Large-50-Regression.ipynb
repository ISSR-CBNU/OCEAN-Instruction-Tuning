{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 只使用 GPU 0\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s]::%(module)s::%(levelname)s::%(message)s')\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "fileHandler = logging.FileHandler('./LOG/Mbart-Large-50-Regression.log')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(streamHandler)\n",
    "logger.addHandler(fileHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-01 17:39:19,863]::3858661222::DEBUG::=============Mbart-Large-50-Regression instruction tuning Strat!=============\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "logger.debug('=============Mbart-Large-50-Regression instruction tuning Strat!=============')\n",
    "\n",
    "df = pd.read_csv(\"/home/oem/qx/in_tuning/KETI_nopreprocessed_labeled_dropdup+seperate_ver0.3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11307, 10)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 크기 확인\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   transcription  \\\n",
      "0  저희가 지금 봐야 되는 게 설치 장소랑 뭘 팔지네요.   \n",
      "1            그러니까 계절이 10월이니까 쌀쌀한   \n",
      "2                          좀 쌀쌀한   \n",
      "3         날씨 그럼 뭐 각자 좀 생각을 해볼까요?   \n",
      "4      아니면 그냥 저희끼리 얘기를 하는 게 편하신지   \n",
      "\n",
      "                                               OCEAN  \n",
      "0  [0.39583333333333337, 0.7708333333333333, 0.72...  \n",
      "1  [0.39583333333333337, 0.7708333333333333, 0.72...  \n",
      "2  [0.7708333333333333, 0.5416666666666666, 0.770...  \n",
      "3  [0.39583333333333337, 0.7708333333333333, 0.72...  \n",
      "4  [0.39583333333333337, 0.7708333333333333, 0.72...  \n"
     ]
    }
   ],
   "source": [
    "df = df[['transcription', 'OCEAN']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "# OCEAN 컬럼을 리스트로 변환 후 소숫점 4자리로 반올림\n",
    "def round_ocean_values(ocean_string):\n",
    "    ocean_list = ast.literal_eval(ocean_string)  # 문자열을 리스트로 변환\n",
    "    return [round(value, 4) for value in ocean_list]  # 각 값을 소숫점 4자리로 반올림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   transcription                                     OCEAN\n",
      "0  저희가 지금 봐야 되는 게 설치 장소랑 뭘 팔지네요.  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]\n",
      "1            그러니까 계절이 10월이니까 쌀쌀한  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]\n",
      "2                          좀 쌀쌀한  [0.7708, 0.5417, 0.7708, 0.8125, 0.3542]\n",
      "3         날씨 그럼 뭐 각자 좀 생각을 해볼까요?  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]\n",
      "4      아니면 그냥 저희끼리 얘기를 하는 게 편하신지  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]\n"
     ]
    }
   ],
   "source": [
    "# OCEAN 값 처리\n",
    "df['OCEAN'] = df['OCEAN'].apply(round_ocean_values)\n",
    "\n",
    "# 확인\n",
    "print(df[['transcription', 'OCEAN']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0  ### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특...   \n",
      "1  ### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특...   \n",
      "2  ### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특...   \n",
      "3  ### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특...   \n",
      "4  ### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특...   \n",
      "\n",
      "                                      OCEAN  \n",
      "0  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]  \n",
      "1  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]  \n",
      "2  [0.7708, 0.5417, 0.7708, 0.8125, 0.3542]  \n",
      "3  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]  \n",
      "4  [0.3958, 0.7708, 0.7292, 0.8333, 0.4375]  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "instruction = \"주어진 대화를 보고, OCEAN 다섯 가지 성격 특성을 0에서 1 사이의 점수로 예측하세요.\"\n",
    "\n",
    "# Instruction + Transcription 결합\n",
    "df['input_text'] = df['transcription'].apply(\n",
    "    lambda x: f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{x}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "# 필요 없는 컬럼 제거\n",
    "df = df[['input_text', 'OCEAN']]\n",
    "\n",
    "print(df.head())  # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'OCEAN'],\n",
      "        num_rows: 9045\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'OCEAN'],\n",
      "        num_rows: 2262\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.shuffle(seed=42)  # 데이터 섞기\n",
    "dataset = dataset.train_test_split(test_size=0.2)  # 80% 학습, 20% 검증\n",
    "\n",
    "# 데이터 확인\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# GPU 사용 시 시드 고정\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# # 选择一个 Seq2Seq 的 XLM-R 类似模型（XLM-R 本身没有 Seq2Seq 版本）\n",
    "# model_name = \"facebook/mbart-large-50\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 选择一个 Seq2Seq 的 XLM-R 类似模型（XLM-R 本身没有 Seq2Seq 版本）\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "class MbartForRegression(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MbartForRegression, self).__init__()\n",
    "        \n",
    "        # loading the full Seq2Seq model\n",
    "        self.Mbart = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # adding a regression head\n",
    "        self.regression_head = nn.Linear(self.Mbart.config.d_model, 5)  # 改为 self.Mbart.config.d_model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # getting the encoder outputs\n",
    "        encoder_outputs = self.Mbart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = encoder_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        regression_output = self.regression_head(last_hidden_state[:, 0, :])\n",
    "\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.L1Loss()\n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            return {\"loss\": loss, \"logits\": regression_output}\n",
    "        \n",
    "        return {\"logits\": regression_output}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 토크나이징 함수 정의\n",
    "def preprocess_data(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # 반환할 딕셔너리 생성input_ids,attention_mask\n",
    "    inputs[\"labels\"] = torch.tensor(examples[\"OCEAN\"], dtype=torch.float32)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da77032494c140f9af5ef35630b3ed5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c22761e889496f918bdcbee5a5d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': '### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특성을 0에서 1 사이의 점수로 예측하세요.\\n\\n### Input:\\n이렇게 돌아다니다가 주류류로 가서 맥주 한 잔하고\\n\\n### Response:', 'OCEAN': [0.8125, 0.75, 0.6042, 0.7083, 0.3125], 'input_ids': [250004, 6, 187284, 72022, 10763, 12, 5837, 2211, 4033, 123541, 688, 30861, 4, 91888, 105621, 6685, 152915, 44276, 182686, 133954, 413, 757, 1180, 106, 62657, 367, 202031, 1083, 211048, 44476, 5, 6, 187284, 360, 7077, 12, 47383, 68862, 98781, 49806, 5837, 18354, 18354, 1083, 170115, 98720, 2688, 3103, 101256, 2382, 6, 187284, 92748, 12, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.8125, 0.75, 0.604200005531311, 0.708299994468689, 0.3125]}\n",
      "Dataset({\n",
      "    features: ['input_text', 'OCEAN', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9045\n",
      "})\n",
      "{'input_text': '### Instruction:\\n주어진 대화를 보고, OCEAN 다섯 가지 성격 특성을 0에서 1 사이의 점수로 예측하세요.\\n\\n### Input:\\n저도 10등이 이종석\\n\\n### Response:', 'OCEAN': [0.5625, 0.3125, 0.6875, 0.6042, 0.1042], 'input_ids': [250004, 6, 187284, 72022, 10763, 12, 5837, 2211, 4033, 123541, 688, 30861, 4, 91888, 105621, 6685, 152915, 44276, 182686, 133954, 413, 757, 1180, 106, 62657, 367, 202031, 1083, 211048, 44476, 5, 6, 187284, 360, 7077, 12, 143023, 209, 19123, 469, 1504, 12286, 14541, 6, 187284, 92748, 12, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.5625, 0.3125, 0.6875, 0.604200005531311, 0.10419999808073044]}\n",
      "Dataset({\n",
      "    features: ['input_text', 'OCEAN', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2262\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 전처리 적용\n",
    "train_dataset = dataset[\"train\"].map(preprocess_data, batched=True)\n",
    "eval_dataset = dataset[\"test\"].map(preprocess_data, batched=True)\n",
    "# 학습용 데이터 확인\n",
    "print(train_dataset[0])\n",
    "print(train_dataset)\n",
    "print(eval_dataset[0])\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "Dataset({\n",
      "    features: ['input_text', 'OCEAN', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2262\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(type(eval_dataset[0][\"OCEAN\"][0]))\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def my_collate_fn(batch):\n",
    "    input_texts = [item['input_text'] for item in batch]\n",
    "    labels = torch.stack([torch.tensor(item['OCEAN']) for item in batch])\n",
    "    return {\"input_text\": input_texts, \"OCEAN\": labels}\n",
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8,collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def evaluate_ocean(model, tokenizer, eval_dataloader, device):\n",
    "    import re\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    model.eval()\n",
    "    all_test_predictions = []\n",
    "    all_test_labels = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        test_input_texts = batch['input_text']\n",
    "        test_labels = batch[\"OCEAN\"]\n",
    "        inputs = tokenizer(test_input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]) \n",
    "            predictions = outputs[\"logits\"] \n",
    "        all_test_predictions.extend(predictions.cpu().numpy().tolist())\n",
    "        all_test_labels.extend(test_labels.cpu().numpy().tolist())\n",
    "    model.train()\n",
    "\n",
    "    mae_score = mean_absolute_error(all_test_labels, all_test_predictions)\n",
    "    one_minus_mae = 1 - mae_score\n",
    "    logger.debug(f\"\\n🧪 Custom Evaluation — 1 - MAE: {one_minus_mae:.4f}\")\n",
    "\n",
    "    return one_minus_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n",
    "    labels = torch.stack([torch.tensor(item['labels']) for item in batch])  # 明确 shape\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState\n",
    "\n",
    "class OceanEvalCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataloader, tokenizer, device, output_dir=\"best_model\"):\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.best_score = -float(\"inf\")\n",
    "\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        model = kwargs['model']\n",
    "        print(\"\\n🔍 Running custom evaluation at end of epoch...\")\n",
    "\n",
    "        score = evaluate_ocean(model, self.tokenizer, self.eval_dataloader, self.device)\n",
    "\n",
    "        if score > self.best_score:\n",
    "            self.best_score = score\n",
    "            logger.debug(f\"💾 New best model found with score {score:.4f} — saving to {self.output_dir}\")\n",
    "\n",
    "            \n",
    "            if not os.path.exists(self.output_dir):\n",
    "                os.makedirs(self.output_dir)\n",
    "            else:\n",
    "                shutil.rmtree(self.output_dir)\n",
    "                os.makedirs(self.output_dir)\n",
    "            # save the best model\n",
    "            torch.save(model.state_dict(), os.path.join(self.output_dir, \"pytorch_model.bin\"))\n",
    "            self.tokenizer.save_pretrained(self.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 17:39:27.869070: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 17:39:27.872756: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 17:39:27.879519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743496767.891384 3651721 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743496767.894715 3651721 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743496767.903622 3651721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743496767.903634 3651721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743496767.903636 3651721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743496767.903637 3651721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 17:39:27.906950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/oem/anaconda3/envs/qx/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3651721/3068156528.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_text', 'OCEAN', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9045\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56550' max='56550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56550/56550 1:34:46, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.03it/s]\n",
      "[2025-04-01 17:41:25,590]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8714\n",
      "[2025-04-01 17:41:25,591]::2281002358::DEBUG::💾 New best model found with score 0.8714 — saving to ./regression_Mbart_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.87it/s]\n",
      "[2025-04-01 17:43:21,953]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8717\n",
      "[2025-04-01 17:43:21,954]::2281002358::DEBUG::💾 New best model found with score 0.8717 — saving to ./regression_Mbart_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.62it/s]\n",
      "[2025-04-01 17:45:17,936]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8737\n",
      "[2025-04-01 17:45:17,937]::2281002358::DEBUG::💾 New best model found with score 0.8737 — saving to ./regression_Mbart_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.52it/s]\n",
      "[2025-04-01 17:47:13,975]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.07it/s]\n",
      "[2025-04-01 17:49:07,707]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8740\n",
      "[2025-04-01 17:49:07,708]::2281002358::DEBUG::💾 New best model found with score 0.8740 — saving to ./regression_Mbart_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.28it/s]\n",
      "[2025-04-01 17:51:03,774]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.17it/s]\n",
      "[2025-04-01 17:52:57,313]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 58.57it/s]\n",
      "[2025-04-01 17:54:50,882]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.48it/s]\n",
      "[2025-04-01 17:56:44,469]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.40it/s]\n",
      "[2025-04-01 17:58:37,923]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.84it/s]\n",
      "[2025-04-01 18:00:31,471]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.91it/s]\n",
      "[2025-04-01 18:02:25,008]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.97it/s]\n",
      "[2025-04-01 18:04:18,429]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.84it/s]\n",
      "[2025-04-01 18:06:11,859]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 57.31it/s]\n",
      "[2025-04-01 18:08:05,575]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.14it/s]\n",
      "[2025-04-01 18:09:58,980]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.54it/s]\n",
      "[2025-04-01 18:11:52,561]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 58.11it/s]\n",
      "[2025-04-01 18:13:46,027]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 61.19it/s]\n",
      "[2025-04-01 18:15:39,557]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.97it/s]\n",
      "[2025-04-01 18:17:33,020]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.82it/s]\n",
      "[2025-04-01 18:19:26,448]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 58.69it/s]\n",
      "[2025-04-01 18:21:20,098]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.18it/s]\n",
      "[2025-04-01 18:23:13,583]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.45it/s]\n",
      "[2025-04-01 18:25:06,963]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.48it/s]\n",
      "[2025-04-01 18:27:00,436]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.45it/s]\n",
      "[2025-04-01 18:28:53,907]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 58.19it/s]\n",
      "[2025-04-01 18:30:47,556]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 61.03it/s]\n",
      "[2025-04-01 18:32:40,892]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.41it/s]\n",
      "[2025-04-01 18:34:34,338]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 56.84it/s]\n",
      "[2025-04-01 18:36:28,127]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.30it/s]\n",
      "[2025-04-01 18:38:21,542]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.22it/s]\n",
      "[2025-04-01 18:40:15,029]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 64.05it/s]\n",
      "[2025-04-01 18:42:08,308]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.18it/s]\n",
      "[2025-04-01 18:44:01,920]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.24it/s]\n",
      "[2025-04-01 18:45:55,394]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 57.70it/s]\n",
      "[2025-04-01 18:47:49,171]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.75it/s]\n",
      "[2025-04-01 18:49:42,664]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.52it/s]\n",
      "[2025-04-01 18:51:36,088]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.85it/s]\n",
      "[2025-04-01 18:53:29,513]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 61.20it/s]\n",
      "[2025-04-01 18:55:23,069]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.40it/s]\n",
      "[2025-04-01 18:57:16,714]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.37it/s]\n",
      "[2025-04-01 18:59:10,097]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 57.74it/s]\n",
      "[2025-04-01 19:01:03,776]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 61.29it/s]\n",
      "[2025-04-01 19:02:57,218]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 59.63it/s]\n",
      "[2025-04-01 19:04:50,772]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 58.44it/s]\n",
      "[2025-04-01 19:06:44,363]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 61.66it/s]\n",
      "[2025-04-01 19:08:37,824]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 60.42it/s]\n",
      "[2025-04-01 19:10:31,233]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 61.67it/s]\n",
      "[2025-04-01 19:12:24,673]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Running custom evaluation at end of epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:04<00:00, 57.54it/s]\n",
      "[2025-04-01 19:14:18,386]::3881085114::DEBUG::\n",
      "🧪 Custom Evaluation — 1 - MAE: 0.8599\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'Mbart.lm_head.weight', 'Mbart.model.decoder.embed_tokens.weight', 'Mbart.model.encoder.embed_tokens.weight', 'Mbart.model.shared.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     39\u001b[0m savefile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./regression_Mbart/epoch50\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msavefile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(savefile)\n",
      "File \u001b[0;32m~/anaconda3/envs/qx/lib/python3.10/site-packages/transformers/trainer.py:3860\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3860\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/anaconda3/envs/qx/lib/python3.10/site-packages/transformers/trainer.py:3958\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3956\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors:\n\u001b[0;32m-> 3958\u001b[0m     \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m   3960\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3962\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n",
      "File \u001b[0;32m~/anaconda3/envs/qx/lib/python3.10/site-packages/safetensors/torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/qx/lib/python3.10/site-packages/safetensors/torch.py:488\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    485\u001b[0m         failing\u001b[38;5;241m.\u001b[39mappend(names)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124m        A potential way to correctly save your model is to use `save_model`.\u001b[39m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    497\u001b[0m     k: {\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    503\u001b[0m }\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'Mbart.lm_head.weight', 'Mbart.model.decoder.embed_tokens.weight', 'Mbart.model.encoder.embed_tokens.weight', 'Mbart.model.shared.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 모델, 데이터셋, 토크나이저 설정\n",
    "# model_name = \"bert-base-uncased\"  # 원하는 모델 사용 가능\n",
    "model = MbartForRegression(model_name)\n",
    "model.to(device)\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./regression_Mbart_results\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"no\",  # 关键设置：禁用 Trainer 自动验证\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False\n",
    "    # fp16=True\n",
    ")\n",
    "\n",
    "callback = OceanEvalCallback(\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    output_dir=\"./regression_Mbart_model\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[callback],\n",
    "    data_collator=my_train_collate_fn\n",
    ")\n",
    "print(train_dataset)\n",
    "# 모델 학습 시작\n",
    "trainer.train()\n",
    "# savefile = \"./regression_Mbart/epoch50\"\n",
    "# trainer.save_model(savefile)\n",
    "# tokenizer.save_pretrained(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoConfig\n",
    "# def load_model_and_tokenizer(model_class, checkpoint_dir):\n",
    "#     # 加载模型权重\n",
    "#     model = model_class(config=AutoConfig.from_pretrained(checkpoint_dir))\n",
    "#     model.load_state_dict(torch.load(f\"{checkpoint_dir}/pytorch_model.bin\"))\n",
    "#     model.eval()  # 设置为推理模式\n",
    "\n",
    "#     # 加载 tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "#     print(f\"✅ 模型与 tokenizer 已加载自: {checkpoint_dir}\")\n",
    "#     return model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
